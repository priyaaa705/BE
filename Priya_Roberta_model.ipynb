{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Priya Roberta model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyaaa705/BE/blob/master/Priya_Roberta_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edFCQvLmhd75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "975188d8-d57e-45ca-ceed-0318bff44de0"
      },
      "source": [
        "!pip install seqeval\n",
        "!pip install pytorch_pretrained_bert\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.1+cu101)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.24)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.24 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.24)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.24->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.24->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.24->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqkVLHA9EdcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from tqdm import trange\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from transformers import RobertaTokenizerFast, RobertaForTokenClassification\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from seqeval.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLIVXIhoPEuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_json('Entity Recognition in Resumes.json', lines=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctg44UOQFpRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2a62c254-1f9e-493b-f418-980eb8749f35"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>annotation</th>\n",
              "      <th>extras</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abhishek Jha\\nApplication Development Associat...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Afreen Jamadar\\nActive member of IIIT Committe...</td>\n",
              "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Akhil Yadav Polemaina\\nHyderabad, Telangana - ...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Alok Khandai\\nOperational Analyst (SQL DBA) En...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...</td>\n",
              "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  ... extras\n",
              "0  Abhishek Jha\\nApplication Development Associat...  ...    NaN\n",
              "1  Afreen Jamadar\\nActive member of IIIT Committe...  ...    NaN\n",
              "2  Akhil Yadav Polemaina\\nHyderabad, Telangana - ...  ...    NaN\n",
              "3  Alok Khandai\\nOperational Analyst (SQL DBA) En...  ...    NaN\n",
              "4  Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...  ...    NaN\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBE33XYqFq3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
        "    try:\n",
        "        training_data = []\n",
        "        lines=[]\n",
        "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            text = data['content'].replace(\"\\n\", \" \")\n",
        "            entities = []\n",
        "            data_annotations = data['annotation']\n",
        "            if data_annotations is not None:\n",
        "                for annotation in data_annotations:\n",
        "                   \n",
        "                    point = annotation['points'][0]\n",
        "                    labels = annotation['label']\n",
        "                   \n",
        "                    if not isinstance(labels, list):\n",
        "                        labels = [labels]\n",
        "\n",
        "                    for label in labels:\n",
        "                        point_start = point['start']\n",
        "                        point_end = point['end']\n",
        "                        point_text = point['text']\n",
        "                        \n",
        "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                        if lstrip_diff != 0:\n",
        "                            point_start = point_start + lstrip_diff\n",
        "                        if rstrip_diff != 0:\n",
        "                            point_end = point_end - rstrip_diff\n",
        "                        entities.append((point_start, point_end + 1 , label))\n",
        "            training_data.append((text, {\"entities\" : entities}))\n",
        "        return training_data\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
        "        return None\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "  invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "  cleaned_data = []\n",
        "  for text, annotations in data:\n",
        "      entities = annotations['entities']\n",
        "      valid_entities = []\n",
        "      for start, end, label in entities:\n",
        "          valid_start = start\n",
        "          valid_end = end\n",
        "          while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                  text[valid_start]):\n",
        "              valid_start += 1\n",
        "          while valid_end > 1 and invalid_span_tokens.match(\n",
        "                  text[valid_end - 1]):\n",
        "              valid_end -= 1\n",
        "          valid_entities.append([valid_start, valid_end, label])\n",
        "      cleaned_data.append([text, {'entities': valid_entities}])\n",
        "  return cleaned_data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKGAGtzzPfMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = trim_entity_spans(convert_dataturks_to_spacy('Entity Recognition in Resumes.json'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRvN2BhmF9BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label(offset, labels):\n",
        "    if offset[0] == 0 and offset[1] == 0:\n",
        "        return 'O'\n",
        "    for label in labels:\n",
        "        if offset[1] >= label[0] and offset[0] <= label[1]:\n",
        "            return label[2]\n",
        "    return 'O'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vb0nLpIGBhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags_vals = [\"O\", \"Name\", \"Degree\",\"Skills\",\"College Name\",\"Email Address\",\"Designation\",\"Companies worked at\",\"Graduation Year\",\"Years of Experience\",\"Location\"]\n",
        "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
        "idx2tag = {i:t for i, t in enumerate(tags_vals)}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlL-BcmJGEQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_resume(data, tokenizer, tag2idx, max_len, is_test=False):\n",
        "    tok = tokenizer.encode_plus(data[0], max_length=max_len, return_offsets_mapping=True, truncation=True)\n",
        "    curr_sent = {'orig_labels':[], 'labels': []}\n",
        "    \n",
        "    padding_length = max_len - len(tok['input_ids'])\n",
        "    \n",
        "    if not is_test:\n",
        "        labels = data[1]['entities']\n",
        "        labels.reverse()\n",
        "        for off in tok['offset_mapping']:\n",
        "            label = get_label(off, labels)\n",
        "            curr_sent['orig_labels'].append(label)\n",
        "            curr_sent['labels'].append(tag2idx[label])\n",
        "        curr_sent['labels'] = curr_sent['labels'] + ([0] * padding_length)\n",
        "    \n",
        "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
        "    curr_sent['attention_mask'] = tok['attention_mask'] + ([0] * padding_length)\n",
        "    return curr_sent\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re_MwA10GICg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResumeDataset(Dataset):\n",
        "    def __init__(self, resume, tokenizer, tag2idx, max_len, is_test=False):\n",
        "        self.resume = resume\n",
        "        self.tokenizer = tokenizer\n",
        "        self.is_test = is_test\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.resume)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        data = process_resume(self.resume[idx], self.tokenizer, self.tag2idx, self.max_len, self.is_test)\n",
        "        return {\n",
        "            'input_ids': torch.tensor(data['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(data['attention_mask'], dtype=torch.long),\n",
        "            'labels': torch.tensor(data['labels'], dtype=torch.long),\n",
        "            'orig_label': data['orig_labels']\n",
        "        }"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQqfZCFsGT-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_hyperparameters(model, ff):\n",
        "\n",
        "    \n",
        "  if ff:\n",
        "      param_optimizer = list(model.named_parameters())\n",
        "      no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
        "      optimizer_grouped_parameters = [\n",
        "          {\n",
        "              \"params\": [\n",
        "                  p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "              ],\n",
        "              \"weight_decay_rate\": 0.01,\n",
        "          },\n",
        "          {\n",
        "              \"params\": [\n",
        "                  p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "              ],\n",
        "              \"weight_decay_rate\": 0.0,\n",
        "          },\n",
        "      ]\n",
        "  else:\n",
        "      param_optimizer = list(model.classifier.named_parameters())\n",
        "      optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "  return optimizer_grouped_parameters"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVkNBlgiGXtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_special_tokens(tokenizer, tag2idx):\n",
        "  vocab = tokenizer.get_vocab()\n",
        "  #pad_tok = vocab[\"[PAD]\"]\n",
        "  #sep_tok = vocab[\"[SEP]\"]\n",
        "  #cls_tok = vocab[\"[CLS]\"]\n",
        "  o_lab = tag2idx[\"O\"]\n",
        "\n",
        "  return o_lab"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rupkIW7vGajX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def annot_confusion_matrix(valid_tags, pred_tags):\n",
        "\n",
        "  \n",
        " \n",
        "    header = sorted(list(set(valid_tags + pred_tags)))\n",
        "\n",
        "    matrix = confusion_matrix(valid_tags, pred_tags, labels=header)\n",
        "\n",
        "    mat_formatted = [header[i] + \"\\t\\t\\t\" + str(row) for i, row in enumerate(matrix)]\n",
        "    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n",
        "\n",
        "    return content"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m05zERSeGdbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_accuracy(valid_tags, pred_tags):\n",
        "  return (np.array(valid_tags) == np.array(pred_tags)).mean()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB-ftkLTHAgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_save_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    optimizer,\n",
        "    epochs,\n",
        "    idx2tag,\n",
        "    tag2idx,\n",
        "    max_grad_norm,\n",
        "    device,\n",
        "    train_dataloader,\n",
        "    valid_dataloader\n",
        "):\n",
        "\n",
        "  o_lab = get_special_tokens(tokenizer, tag2idx)\n",
        "\n",
        "  epoch = 0\n",
        "  for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    epoch += 1\n",
        "\n",
        "    print(\"Starting training loop.\")\n",
        "    model.train()\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        \n",
        "        b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
        "        b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
        "\n",
        "        \n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_input_mask,\n",
        "            labels=b_labels,\n",
        "        )\n",
        "        loss, tr_logits = outputs[:2]\n",
        "\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "      \n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        \n",
        "        preds_mask = (\n",
        "            (b_input_ids != o_lab)\n",
        "            #& (b_input_ids != pad_tok)\n",
        "            #& (b_input_ids != sep_tok)\n",
        "        )\n",
        "        #preds_mask = 1\n",
        "        tr_logits = tr_logits.cpu().detach().numpy()\n",
        "        tr_label_ids = torch.masked_select(b_labels, (preds_mask==1))\n",
        "        preds_mask = preds_mask.cpu().detach().numpy()\n",
        "        tr_batch_preds = np.argmax(tr_logits[preds_mask.squeeze()], axis=1)\n",
        "        tr_batch_labels = tr_label_ids.to(\"cpu\").numpy()\n",
        "        tr_preds.extend(tr_batch_preds)\n",
        "        tr_labels.extend(tr_batch_labels)\n",
        "\n",
        "    \n",
        "        tmp_tr_accuracy = flat_accuracy(tr_batch_labels, tr_batch_preds)\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=max_grad_norm\n",
        "        )\n",
        "\n",
        "        \n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "    tr_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "\n",
        "    \n",
        "    print(f\"Train loss: {tr_loss}\")\n",
        "    print(f\"Train accuracy: {tr_accuracy}\")\n",
        "    \n",
        "    \n",
        "\n",
        "    print(\"Starting validation loop.\")\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in valid_dataloader:\n",
        "\n",
        "        b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
        "        b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                attention_mask=b_input_mask,\n",
        "                labels=b_labels,\n",
        "            )\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "        \n",
        "        preds_mask = (\n",
        "            (b_input_ids != o_lab)\n",
        "            #& (b_input_ids != pad_tok)\n",
        "            #& (b_input_ids != sep_tok)\n",
        "        )\n",
        "\n",
        "        logits = logits.cpu().detach().numpy()\n",
        "        label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
        "        preds_mask = preds_mask.cpu().detach().numpy()\n",
        "        val_batch_preds = np.argmax(logits[preds_mask.squeeze()], axis=1)\n",
        "        val_batch_labels = label_ids.to(\"cpu\").numpy()\n",
        "        predictions.extend(val_batch_preds)\n",
        "        true_labels.extend(val_batch_labels)\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(val_batch_labels, val_batch_preds)\n",
        "\n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        nb_eval_examples += b_input_ids.size(0)\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    \n",
        "    pred_tags = [idx2tag[i] for i in predictions]\n",
        "    valid_tags = [idx2tag[i] for i in true_labels]\n",
        "    cl_report = classification_report(valid_tags, pred_tags)\n",
        "    conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "\n",
        "    \n",
        "    #print(f\"Validation loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "    #print(f\"Classification Report:\\n {cl_report}\")\n",
        "    #print(f\"Confusion Matrix:\\n {conf_mat}\")\n",
        "\n",
        "  return(pred_tags,valid_tags,eval_accuracy)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxaWtIrPFJcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 500\n",
        "EPOCHS = 5\n",
        "MODEL_PATH = '../input/roberta-base'\n",
        "TOKENIZER = RobertaTokenizerFast.from_pretrained('roberta-base',lowercase=True)\n",
        "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YterZ03gyCDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "63391631-35de-41f9-b6f7-3d41e4a90296"
      },
      "source": [
        "train_data = data[:132] + data[176:]\n",
        "val_data = data[132:176]\n",
        "total = len(data)\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "train_d = ResumeDataset(train_data, TOKENIZER, tag2idx, MAX_LEN)\n",
        "val_d = ResumeDataset(val_data, TOKENIZER, tag2idx, MAX_LEN)\n",
        "\n",
        "train_sampler = RandomSampler(train_d)\n",
        "train_dl = DataLoader(train_d, sampler=train_sampler, batch_size=8)\n",
        "val_dl = DataLoader(val_d, batch_size=4)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=len(tag2idx))\n",
        "model.to(DEVICE);\n",
        "optimizer_grouped_parameters = get_hyperparameters(model, True)\n",
        "optimizer = Adam(optimizer_grouped_parameters,lr=3e-5)\n",
        "\n",
        "pred_tags, valid_tags,eval_accuracy = train_and_save_model(\n",
        "  model, \n",
        "  TOKENIZER, \n",
        "  optimizer, \n",
        "  EPOCHS, \n",
        "  idx2tag, \n",
        "  tag2idx, \n",
        "  MAX_GRAD_NORM, \n",
        "  DEVICE, \n",
        "  train_dl, \n",
        "  val_dl\n",
        ")\n",
        "\n",
        "#oos_y.append(valid_tags)\n",
        "#oos_pred.append(pred_tags)\n",
        "#pred = np.argmax(pred_tags)\n",
        "#y_compare = np.argmax(valid_tags)\n",
        "#score = metrics.accuracy_score(y_compare,pred)\n",
        "print(f'fold score (accuracy): {eval_accuracy}\\n\\n')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting training loop.\n",
            "Train loss: 0.9291560514406725\n",
            "Train accuracy: 0.778744611272716\n",
            "Starting validation loop.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 1/5 [00:18<01:15, 18.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8717533827530094\n",
            "Starting training loop.\n",
            "Train loss: 0.44004582545974036\n",
            "Train accuracy: 0.8599388762061672\n",
            "Starting validation loop.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [00:37<00:56, 18.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8864721847453715\n",
            "Starting training loop.\n",
            "Train loss: 0.30877685750072653\n",
            "Train accuracy: 0.8965588544780648\n",
            "Starting validation loop.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [00:57<00:38, 19.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9281572674561432\n",
            "Starting training loop.\n",
            "Train loss: 0.21479990942911667\n",
            "Train accuracy: 0.929252421574822\n",
            "Starting validation loop.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [01:16<00:19, 19.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.936368475394044\n",
            "Starting training loop.\n",
            "Train loss: 0.1756694530221549\n",
            "Train accuracy: 0.9420293489748418\n",
            "Starting validation loop.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 5/5 [01:36<00:00, 19.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9365946055545001\n",
            "fold score (accuracy): 0.9365946055545001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkoaSGGuic78",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d5ac8e7-5556-4740-afb0-7b010a234f16"
      },
      "source": [
        "print(val_d[0])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([    0, 29880, 21639,   256,  7363,   257,  2478,  5454, 35201,  4827,\n",
            "         1589, 11274,   337,  1258,  4827,   111,  3709,   666,  1437, 17502,\n",
            "         7826,     6,  5477,  1097,  1113,   111,  3593,   162,    15,  7908,\n",
            "           35,  5329,     4,   175,    73,   338,    73, 29880, 21639,    12,\n",
            "          256,  7363,   257,  2478,    73,   288, 28690,   438,   246, 32769,\n",
            "        36920,   134,   428,  1610,   873,  1437, 32008, 10649, 21260, 41499,\n",
            "         1437,  5454, 35201,  4827,  1589, 11274,   337,  1258,  4827,  1437,\n",
            "         3709,   666,   111,  1437,   502,   336,     7, 17356,  1437,   502,\n",
            "          336,   111, 15628,  1248,  1437,   248,  7003,     8, 25714, 38935,\n",
            "           35,  1347, 11214,    25,    10,  5454, 35201,  4827,  1589, 11274,\n",
            "          337,  1258,  4827,    13,  6253,  2068,    11,  3709,     4, 11518,\n",
            "         5941,   680,   981,   165,  1414,     6,  4196,   165,  1915,     6,\n",
            "          981,   359,  3914,   131,  1428,   239,   913,  3165,  4495,     7,\n",
            "         3547,     4, 13786,  8231,  1031,  3496,  1328,     5,  1160,  6612,\n",
            "         3547,     6,  8469,     7,   720,   916,     6,   949,  4585,    73,\n",
            "          534, 13123,    15,  2256,  4495,     6, 23373,  1313,     8, 10358,\n",
            "         7762,    30, 15190,     8, 15704,  4495,     7,  3901,  1915,    77,\n",
            "          956,     4, 13915,  5867,   618,   538,  1160,   690,     8,   229,\n",
            "        11337,   690,     9, 11274,   337,  1258,  1753,    13,  3708,  6173,\n",
            "            4,  1347,  5427,     8,   694, 10944,   672,  3779,   323,    13,\n",
            "         1768,  1904,   464,    73, 20692,  4031,  1061,   420,  1651,     4,\n",
            "         1347, 15530,   165,     9,   291,   226,   134,    18,     8,  4196,\n",
            "         1230,  1414,     4,  1347, 20538,   289, 20885,  2891,    13,   165,\n",
            "           15,  1230,   819,  3247,     8,  3616,   183,     7,   183,  3294,\n",
            "         1713,     4,  1347, 44639,  1295,     5,   829, 18145,   913,     8,\n",
            "         3784,     5,  3308,     7,     5,   235, 17866,    19,  4692, 23607,\n",
            "            7, 15292,     5,   696,    23,     5, 13342,     8,  2969,     5,\n",
            "         3709,  3425,  1337,   544,  2301,    61,    16,   145,  2132,    11,\n",
            "          645,     7,  4949,     5,  3901,  1915,     7,   244,  1305,  3547,\n",
            "            4,  1347,  1783,    25,   720, 18145,   323,     7, 18251,    19,\n",
            "         1337,   720,   893,   624,  3709,  3779,     8,  1305,  4679,  1519,\n",
            "           15,   239,  3887,  4495,   215,    25,  1554,  4628,   221,   134,\n",
            "          359,  3914,   131,   221,   176,  4495,     7,  7057,     5,   518,\n",
            "           25,  1010,    25,   678,     8,  2142,  2526,  4372,     7,   720,\n",
            "          916,    15,  2194,     9,     5,  1160,     6,   381,  3847,     6,\n",
            "            8,   595, 12300,   563,     4,  1347, 11209,     5, 14241, 35296,\n",
            "            9,    70,  4495,     7,  7057,  2340,   544,  2513,    25,  1335,\n",
            "           25,   678,     8, 15925,     5, 12661,   913,    15,   265,  1414,\n",
            "          420,  7183,     6,  4634,  7540,  5206,    14,     5,   275,   678,\n",
            "         1389,     9,   544,  1318,     8,  7265,    32,  4925,     4,  1347,\n",
            "        23853,     5,  3682,   154,   467,     8,  9311,     5,  4372,    61,\n",
            "           40,    28,  1051,     7,   723,  1052,    13,    70,   538,  4495,\n",
            "           73,   995,  3443,     4,  1347,  2184,     5, 11274,   337,  1258,\n",
            "          609,     8,   489,    62,    19,     5, 10962,   250,    18,    15,\n",
            "           70,     5,    92, 11274,   337,  1258,  3308,     8,  3780,  1160,\n",
            "        24600,  4383,     8, 31583,     8,   323,  1134,    13,  5454,  1160,\n",
            "         3547,     4,  1347, 18337,  5530,   374,    12,   133,    12, 43128,\n",
            "           36,   673,   863,   565,    43,  1058,    13,     5,  3862,  4547,\n",
            "         2903,    29,     4,  1347, 13915,  5867,  4114,   529,  4026,    61,\n",
            "         2386,     7,  1477, 13141,   420,   893,     8,     7,  2268,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([ 0,  1,  1,  1,  1,  1,  1,  6,  6,  6,  6,  6,  6,  6,  6,  0,  7,  7,\n",
            "         0, 10, 10, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  0,  0,  0,  0,  0,  0,  6,  6,  6,  6,  6,  6,  6,  6,  0,  7,  7,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  6,  6,  6,  6,  6,  6,  6,  6,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), 'orig_label': ['O', 'Name', 'Name', 'Name', 'Name', 'Name', 'Name', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'O', 'Companies worked at', 'Companies worked at', 'O', 'Location', 'Location', 'Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'O', 'O', 'O', 'O', 'O', 'O', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'O', 'Companies worked at', 'Companies worked at', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS5MfCuujAM4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eeba3bf4-47b4-4aea-80bb-55801ace5498"
      },
      "source": [
        "type(model)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.modeling_roberta.RobertaForTokenClassification"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAoyZKVojGJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "88d43093-9d36-4424-d0cd-5c5cfe17f2ae"
      },
      "source": [
        "Pkl_Filename = \"trained_model.pkl\"  \n",
        "\n",
        "with open(Pkl_Filename, 'wb') as file:  \n",
        "    pickle.dump(model, file)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3SrnCo2jR7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c591498-8eae-4ef2-9f9d-3706a9ff8cec"
      },
      "source": [
        "with open(Pkl_Filename, 'rb') as file:  \n",
        "    model = pickle.load(file)\n",
        "\n",
        "type(model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.modeling_roberta.RobertaForTokenClassification"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOqywfBQjXC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "88678b9b-92f5-4195-c17f-dc30e81e1b39"
      },
      "source": [
        "!pip install PyMuPDF"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyMuPDF\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/32/39085b2f6cfcac5d4ff10bd3b9527de193b32960ba6a0fd2afb3a5c412c2/PyMuPDF-1.17.4-cp36-cp36m-manylinux2010_x86_64.whl (6.0MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0MB 4.0MB/s \n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.17.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX9IUgyojcfY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d97fe673-8a70-4aff-b70a-1c39bdd38506"
      },
      "source": [
        "5+3"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHbQ5PKc1OlX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "e6b08225-6b21-41d7-f76c-f522afd77a8c"
      },
      "source": [
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "'''"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive\\')\\ndrive.mount(\"/content/drive\", force_remount=True)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfEYc6zu1NoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "389aabfa-4735-4f02-faee-63df3163c555"
      },
      "source": [
        "'''\n",
        "path = '/content/drive/My Drive/'\n",
        "torch.save(\n",
        "    {\n",
        "        \"epoch\": EPOCHS,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    path + 'model_e6.tar',\n",
        ")\n",
        "'''"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\npath = \\'/content/drive/My Drive/\\'\\ntorch.save(\\n    {\\n        \"epoch\": EPOCHS,\\n        \"model_state_dict\": model.state_dict(),\\n        \"optimizer_state_dict\": optimizer.state_dict(),\\n    },\\n    path + \\'model_e6.tar\\',\\n)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGomHrhxBeWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "de31938f-083d-4e40-be88-eaa60881f4ef"
      },
      "source": [
        "'''\n",
        "model = torch.load('model_e6.tar')\n",
        "'''"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nmodel = torch.load('model_e6.tar')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NQ1xOepBg11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}