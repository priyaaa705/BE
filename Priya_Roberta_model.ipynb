{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Priya Roberta model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyaaa705/BE/blob/master/Priya_Roberta_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edFCQvLmhd75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "b991dc0d-5c0d-41bb-997a-b4b30e407f9a"
      },
      "source": [
        "!pip install seqeval\n",
        "!pip install pytorch_pretrained_bert\n",
        "!pip install transformers"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.1+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.24 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.24)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.24->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.24->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.24->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqkVLHA9EdcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import trange\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from transformers import RobertaTokenizerFast, RobertaForTokenClassification\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from seqeval.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLIVXIhoPEuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_json('Entity Recognition in Resumes.json', lines=True)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctg44UOQFpRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7b13dfe6-5d57-44bc-d60c-aeaf3984e6fe"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>annotation</th>\n",
              "      <th>extras</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abhishek Jha\\nApplication Development Associat...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Afreen Jamadar\\nActive member of IIIT Committe...</td>\n",
              "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Akhil Yadav Polemaina\\nHyderabad, Telangana - ...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Alok Khandai\\nOperational Analyst (SQL DBA) En...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...</td>\n",
              "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  ... extras\n",
              "0  Abhishek Jha\\nApplication Development Associat...  ...    NaN\n",
              "1  Afreen Jamadar\\nActive member of IIIT Committe...  ...    NaN\n",
              "2  Akhil Yadav Polemaina\\nHyderabad, Telangana - ...  ...    NaN\n",
              "3  Alok Khandai\\nOperational Analyst (SQL DBA) En...  ...    NaN\n",
              "4  Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...  ...    NaN\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBE33XYqFq3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
        "    try:\n",
        "        training_data = []\n",
        "        lines=[]\n",
        "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            text = data['content'].replace(\"\\n\", \" \")\n",
        "            entities = []\n",
        "            data_annotations = data['annotation']\n",
        "            if data_annotations is not None:\n",
        "                for annotation in data_annotations:\n",
        "                   \n",
        "                    point = annotation['points'][0]\n",
        "                    labels = annotation['label']\n",
        "                   \n",
        "                    if not isinstance(labels, list):\n",
        "                        labels = [labels]\n",
        "\n",
        "                    for label in labels:\n",
        "                        point_start = point['start']\n",
        "                        point_end = point['end']\n",
        "                        point_text = point['text']\n",
        "                        \n",
        "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                        if lstrip_diff != 0:\n",
        "                            point_start = point_start + lstrip_diff\n",
        "                        if rstrip_diff != 0:\n",
        "                            point_end = point_end - rstrip_diff\n",
        "                        entities.append((point_start, point_end + 1 , label))\n",
        "            training_data.append((text, {\"entities\" : entities}))\n",
        "        return training_data\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
        "        return None\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "  invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "  cleaned_data = []\n",
        "  for text, annotations in data:\n",
        "      entities = annotations['entities']\n",
        "      valid_entities = []\n",
        "      for start, end, label in entities:\n",
        "          valid_start = start\n",
        "          valid_end = end\n",
        "          while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                  text[valid_start]):\n",
        "              valid_start += 1\n",
        "          while valid_end > 1 and invalid_span_tokens.match(\n",
        "                  text[valid_end - 1]):\n",
        "              valid_end -= 1\n",
        "          valid_entities.append([valid_start, valid_end, label])\n",
        "      cleaned_data.append([text, {'entities': valid_entities}])\n",
        "  return cleaned_data"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKGAGtzzPfMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = trim_entity_spans(convert_dataturks_to_spacy('Entity Recognition in Resumes.json'))"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRvN2BhmF9BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label(offset, labels):\n",
        "    if offset[0] == 0 and offset[1] == 0:\n",
        "        return 'O'\n",
        "    for label in labels:\n",
        "        if offset[1] >= label[0] and offset[0] <= label[1]:\n",
        "            return label[2]\n",
        "    return 'O'"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vb0nLpIGBhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags_vals = [\"O\", \"Name\", \"Degree\",\"Skills\",\"College Name\",\"Email Address\",\"Designation\",\"Companies worked at\",\"Graduation Year\",\"Years of Experience\",\"Location\"]\n",
        "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
        "idx2tag = {i:t for i, t in enumerate(tags_vals)}"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlL-BcmJGEQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_resume(data, tokenizer, tag2idx, max_len, is_test=False):\n",
        "    tok = tokenizer.encode_plus(data[0], max_length=max_len, return_offsets_mapping=True, truncation=True)\n",
        "    curr_sent = {'orig_labels':[], 'labels': []}\n",
        "    \n",
        "    padding_length = max_len - len(tok['input_ids'])\n",
        "    \n",
        "    if not is_test:\n",
        "        labels = data[1]['entities']\n",
        "        labels.reverse()\n",
        "        for off in tok['offset_mapping']:\n",
        "            label = get_label(off, labels)\n",
        "            curr_sent['orig_labels'].append(label)\n",
        "            curr_sent['labels'].append(tag2idx[label])\n",
        "        curr_sent['labels'] = curr_sent['labels'] + ([0] * padding_length)\n",
        "    \n",
        "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
        "    curr_sent['attention_mask'] = tok['attention_mask'] + ([0] * padding_length)\n",
        "    return curr_sent\n"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re_MwA10GICg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResumeDataset(Dataset):\n",
        "    def __init__(self, resume, tokenizer, tag2idx, max_len, is_test=False):\n",
        "        self.resume = resume\n",
        "        self.tokenizer = tokenizer\n",
        "        self.is_test = is_test\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.resume)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        data = process_resume(self.resume[idx], self.tokenizer, self.tag2idx, self.max_len, self.is_test)\n",
        "        return {\n",
        "            'input_ids': torch.tensor(data['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(data['attention_mask'], dtype=torch.long),\n",
        "            'labels': torch.tensor(data['labels'], dtype=torch.long),\n",
        "            'orig_label': data['orig_labels']\n",
        "        }"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQqfZCFsGT-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_hyperparameters(model, ff):\n",
        "\n",
        "    \n",
        "  if ff:\n",
        "      param_optimizer = list(model.named_parameters())\n",
        "      no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
        "      optimizer_grouped_parameters = [\n",
        "          {\n",
        "              \"params\": [\n",
        "                  p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "              ],\n",
        "              \"weight_decay_rate\": 0.01,\n",
        "          },\n",
        "          {\n",
        "              \"params\": [\n",
        "                  p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "              ],\n",
        "              \"weight_decay_rate\": 0.0,\n",
        "          },\n",
        "      ]\n",
        "  else:\n",
        "      param_optimizer = list(model.classifier.named_parameters())\n",
        "      optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "  return optimizer_grouped_parameters"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVkNBlgiGXtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_special_tokens(tokenizer, tag2idx):\n",
        "  vocab = tokenizer.get_vocab()\n",
        "  #pad_tok = vocab[\"[PAD]\"]\n",
        "  #sep_tok = vocab[\"[SEP]\"]\n",
        "  #cls_tok = vocab[\"[CLS]\"]\n",
        "  o_lab = tag2idx[\"O\"]\n",
        "\n",
        "  return o_lab"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rupkIW7vGajX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def annot_confusion_matrix(valid_tags, pred_tags):\n",
        "\n",
        "  \n",
        " \n",
        "    header = sorted(list(set(valid_tags + pred_tags)))\n",
        "\n",
        "    matrix = confusion_matrix(valid_tags, pred_tags, labels=header)\n",
        "\n",
        "    mat_formatted = [header[i] + \"\\t\\t\\t\" + str(row) for i, row in enumerate(matrix)]\n",
        "    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n",
        "\n",
        "    return content"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m05zERSeGdbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_accuracy(valid_tags, pred_tags):\n",
        "  return (np.array(valid_tags) == np.array(pred_tags)).mean()"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB-ftkLTHAgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_save_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    optimizer,\n",
        "    epochs,\n",
        "    idx2tag,\n",
        "    tag2idx,\n",
        "    max_grad_norm,\n",
        "    device,\n",
        "    train_dataloader,\n",
        "    valid_dataloader\n",
        "):\n",
        "\n",
        "  o_lab = get_special_tokens(tokenizer, tag2idx)\n",
        "\n",
        "  epoch = 0\n",
        "  for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    epoch += 1\n",
        "\n",
        "    print(\"Starting training loop.\")\n",
        "    model.train()\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        \n",
        "        b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
        "        b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
        "\n",
        "        \n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_input_mask,\n",
        "            labels=b_labels,\n",
        "        )\n",
        "        loss, tr_logits = outputs[:2]\n",
        "\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "      \n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        \n",
        "        preds_mask = (\n",
        "            (b_input_ids != o_lab)\n",
        "            #& (b_input_ids != pad_tok)\n",
        "            #& (b_input_ids != sep_tok)\n",
        "        )\n",
        "        #preds_mask = 1\n",
        "        tr_logits = tr_logits.cpu().detach().numpy()\n",
        "        tr_label_ids = torch.masked_select(b_labels, (preds_mask==1))\n",
        "        preds_mask = preds_mask.cpu().detach().numpy()\n",
        "        tr_batch_preds = np.argmax(tr_logits[preds_mask.squeeze()], axis=1)\n",
        "        tr_batch_labels = tr_label_ids.to(\"cpu\").numpy()\n",
        "        tr_preds.extend(tr_batch_preds)\n",
        "        tr_labels.extend(tr_batch_labels)\n",
        "\n",
        "    \n",
        "        tmp_tr_accuracy = flat_accuracy(tr_batch_labels, tr_batch_preds)\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=max_grad_norm\n",
        "        )\n",
        "\n",
        "        \n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "    tr_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "\n",
        "    \n",
        "    print(f\"Train loss: {tr_loss}\")\n",
        "    print(f\"Train accuracy: {tr_accuracy}\")\n",
        "    \n",
        "    \n",
        "\n",
        "    print(\"Starting validation loop.\")\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in valid_dataloader:\n",
        "\n",
        "        b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
        "        b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                attention_mask=b_input_mask,\n",
        "                labels=b_labels,\n",
        "            )\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "        \n",
        "        preds_mask = (\n",
        "            (b_input_ids != o_lab)\n",
        "            #& (b_input_ids != pad_tok)\n",
        "            #& (b_input_ids != sep_tok)\n",
        "        )\n",
        "\n",
        "        logits = logits.cpu().detach().numpy()\n",
        "        label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
        "        preds_mask = preds_mask.cpu().detach().numpy()\n",
        "        val_batch_preds = np.argmax(logits[preds_mask.squeeze()], axis=1)\n",
        "        val_batch_labels = label_ids.to(\"cpu\").numpy()\n",
        "        predictions.extend(val_batch_preds)\n",
        "        true_labels.extend(val_batch_labels)\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(val_batch_labels, val_batch_preds)\n",
        "\n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        nb_eval_examples += b_input_ids.size(0)\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    \n",
        "    pred_tags = [idx2tag[i] for i in predictions]\n",
        "    valid_tags = [idx2tag[i] for i in true_labels]\n",
        "    cl_report = classification_report(valid_tags, pred_tags)\n",
        "    conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "\n",
        "    \n",
        "    #print(f\"Validation loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "    #print(f\"Classification Report:\\n {cl_report}\")\n",
        "    #print(f\"Confusion Matrix:\\n {conf_mat}\")\n",
        "\n",
        "  return(pred_tags,valid_tags,eval_accuracy)\n"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxaWtIrPFJcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 500\n",
        "EPOCHS = 5\n",
        "MODEL_PATH = '../input/roberta-base'\n",
        "TOKENIZER = RobertaTokenizerFast.from_pretrained('roberta-base',lowercase=True)\n",
        "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YterZ03gyCDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "49931289-0f46-4a56-eaa2-efb0e587d6eb"
      },
      "source": [
        "train_data = data[:132] + data[176:]\n",
        "val_data = data[132:176]\n",
        "total = len(data)\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "train_d = ResumeDataset(train_data, TOKENIZER, tag2idx, MAX_LEN)\n",
        "val_d = ResumeDataset(val_data, TOKENIZER, tag2idx, MAX_LEN)\n",
        "\n",
        "train_sampler = RandomSampler(train_d)\n",
        "train_dl = DataLoader(train_d, sampler=train_sampler, batch_size=8)\n",
        "val_dl = DataLoader(val_d, batch_size=4)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=len(tag2idx))\n",
        "model.to(DEVICE);\n",
        "optimizer_grouped_parameters = get_hyperparameters(model, True)\n",
        "optimizer = Adam(optimizer_grouped_parameters,lr=3e-5)\n",
        "\n",
        "pred_tags, valid_tags,eval_accuracy = train_and_save_model(\n",
        "  model, \n",
        "  TOKENIZER, \n",
        "  optimizer, \n",
        "  EPOCHS, \n",
        "  idx2tag, \n",
        "  tag2idx, \n",
        "  MAX_GRAD_NORM, \n",
        "  DEVICE, \n",
        "  train_dl, \n",
        "  val_dl\n",
        ")\n",
        "\n",
        "#oos_y.append(valid_tags)\n",
        "#oos_pred.append(pred_tags)\n",
        "#pred = np.argmax(pred_tags)\n",
        "#y_compare = np.argmax(valid_tags)\n",
        "#score = metrics.accuracy_score(y_compare,pred)\n",
        "print(f'fold score (accuracy): {eval_accuracy}\\n\\n')"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:transformers.modeling_utils:Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting training loop.\n",
            "Train loss: 0.9440876651893962\n",
            "Train accuracy: 0.7627678486349393\n",
            "Starting validation loop.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 1/5 [00:18<01:15, 18.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.865280705035885\n",
            "Starting training loop.\n",
            "Train loss: 0.43012277307835495\n",
            "Train accuracy: 0.8646527898961441\n",
            "Starting validation loop.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [00:37<00:56, 18.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9028813118636015\n",
            "Starting training loop.\n",
            "Train loss: 0.2747299535707994\n",
            "Train accuracy: 0.9116976844227905\n",
            "Starting validation loop.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [00:57<00:38, 19.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9137785403875239\n",
            "Starting training loop.\n",
            "Train loss: 0.2526176382194866\n",
            "Train accuracy: 0.9125694614498517\n",
            "Starting validation loop.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [01:16<00:19, 19.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9228345228160136\n",
            "Starting training loop.\n",
            "Train loss: 0.18643423508514056\n",
            "Train accuracy: 0.9379778606464387\n",
            "Starting validation loop.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 5/5 [01:36<00:00, 19.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9256204278805732\n",
            "fold score (accuracy): 0.9256204278805732\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHbQ5PKc1OlX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a01b86f9-9767-407a-c1ed-4c17ebbc9bd0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfEYc6zu1NoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/'\n",
        "torch.save(\n",
        "    {\n",
        "        \"epoch\": EPOCHS,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    path + 'model_e6.tar',\n",
        ")"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGomHrhxBeWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load('model_e6.tar')"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NQ1xOepBg11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}